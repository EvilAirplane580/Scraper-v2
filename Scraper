import psycopg2
from psycopg2.pool import ThreadedConnectionPool
from psycopg2.extras import execute_batch
import requests
from bs4 import BeautifulSoup
import re
from urllib.parse import urljoin, urlparse
from urllib import robotparser
from datetime import datetime
import time
import random
from tqdm import tqdm
import signal
import sys
import logging
import json
import threading
import sqlite3
from contextlib import contextmanager
from typing import List, Dict, Optional, Any, Set
from prometheus_client import start_http_server, Counter, Histogram, Gauge

# ========================
#      Configuration
# ========================
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('scraper.log'),
        logging.StreamHandler()
    ]
)

# ========================
#      Prometheus Metrics
# ========================
REQUESTS_TOTAL = Counter('http_requests_total', 'Total requests made', ['domain', 'status'])
FINDINGS_TOTAL = Counter('secrets_found_total', 'Secrets found by type', ['type'])
CIRCUIT_STATE = Gauge('circuit_state', 'Circuit breaker state (0=closed, 1=open)', ['domain'])
REQUEST_DURATION = Histogram('request_duration_seconds', 'Request duration', ['domain'])
QUEUE_SIZE = Gauge('url_queue_size', 'Current URL queue size')

class CircuitBreaker:
    def __init__(self, threshold=5, reset_timeout=60):
        self.threshold = threshold
        self.reset_timeout = reset_timeout
        self.failures = 0
        self.last_failure = 0
        self._lock = threading.Lock()

    def should_block(self):
        with self._lock:
            if self.failures < self.threshold:
                return False
            if (time.time() - self.last_failure) > self.reset_timeout:
                self.failures = 0
                return False
            return True

    def record_failure(self):
        with self._lock:
            if (time.time() - self.last_failure) > self.reset_timeout:
                self.failures = 0
            self.failures += 1
            self.last_failure = time.time()

class BugBountyScraper:
    def __init__(
        self, 
        db_config: dict, 
        allowed_domains: Optional[Set[str]] = None,
        use_proxies=False, 
        proxy_list=None
    ):
        start_http_server(8000)
        self.db_config = db_config
        self._init_db()
        self._shutdown_lock = threading.Lock()
        
        # Domain restrictions and ToS tracking
        self.allowed_domains = {d.lower() for d in allowed_domains} if allowed_domains else None
        self.tos_allow = {}
        self.tos_patterns = [
            re.compile(phrase, re.IGNORECASE) for phrase in [
                r'\bno\s+robots?\b',
                r'\bno\s+automated\s+access\b',
                r'\bno\s+scraping\b',
                r'\bno\s+crawling\b',
                r'\b禁止机器人\b',
                r'\b禁止爬虫\b'
            ]
        ]

        self.visited_db = sqlite3.connect(':memory:', check_same_thread=False)
        self.visited_db.execute("CREATE TABLE urls(url TEXT PRIMARY KEY)")
        
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 BugBountyScanner/4.0',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'DNT': '1'
        })
        
        self.use_proxies = use_proxies
        self.proxy_list = proxy_list or []
        self.current_proxy = None
        self.proxy_failures = 0
        
        self.sensitive_patterns = {
            'api_key': re.compile(r"(?:(?:api|secret)[_-]?key|token)[=:]\s*['\"]([A-Za-z0-9]{20,})['\"]", re.I),
            'jwt': re.compile(r"(eyJ[a-zA-Z0-9_-]+\.[a-zA-Z0-9_-]+\.[a-zA-Z0-9_-]*)"),
            'aws_key': re.compile(r"((AKIA|ASIA|ABIA|ACCA)[0-9A-Z]{16})"),
        }

        self.circuit_breakers = {}
        self.url_queue = []
        self.findings_buffer = []
        self.stats = {
            'start_time': datetime.now(),
            'requests_made': 0,
            'pages_scraped': 0,
            'secrets_found': 0,
            'max_pages': 5000
        }

        signal.signal(signal.SIGINT, self._shutdown_handler)

    def _init_db(self):
        self.pool = ThreadedConnectionPool(1, 10, **self.db_config)
        with self.db_cursor() as cursor:
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS targets (
                    id SERIAL PRIMARY KEY,
                    url TEXT NOT NULL UNIQUE,
                    domain TEXT NOT NULL,
                    status TEXT DEFAULT 'pending',
                    discovered_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    scanned_at TIMESTAMP NULL
                );
                CREATE INDEX IF NOT EXISTS targets_url_idx ON targets(url);
                CREATE INDEX IF NOT EXISTS targets_status_idx ON targets(status);
                
                CREATE TABLE IF NOT EXISTS findings (
                    id SERIAL PRIMARY KEY,
                    target_id INTEGER REFERENCES targets(id),
                    type TEXT NOT NULL,
                    match TEXT NOT NULL,
                    context TEXT,
                    content_type TEXT,
                    source_url TEXT NOT NULL,
                    severity TEXT NOT NULL,
                    discovered_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                );
                
                CREATE TABLE IF NOT EXISTS scan_metadata (
                    scan_id SERIAL PRIMARY KEY,
                    start_time TIMESTAMP NOT NULL,
                    end_time TIMESTAMP NULL,
                    pages_scraped INTEGER DEFAULT 0,
                    findings_count INTEGER DEFAULT 0
                );
            """)

    @contextmanager
    def db_cursor(self):
        conn = self.pool.getconn()
        try:
            with conn.cursor() as cursor:
                yield cursor
            conn.commit()
        except Exception as e:
            conn.rollback()
            raise
        finally:
            self.pool.putconn(conn)

    def is_allowed_domain(self, url: str) -> bool:
        """Check if domain is in allowed list (case-insensitive)"""
        if self.allowed_domains is None:
            return True
        domain = urlparse(url).netloc.lower()
        return domain in self.allowed_domains

    def should_visit(self, url: str) -> bool:
        """Enhanced URL validation with domain and ToS checks"""
        if not self.is_valid_url(url):
            return False
            
        parsed = urlparse(url)
        domain = parsed.netloc.lower()
        
        # File extension check
        if any(parsed.path.lower().endswith(ext) for ext in 
              ('.jpg', '.png', '.gif', '.pdf', '.zip', '.exe', '.mp4')):
            return False

        # Domain allow list check
        if not self.is_allowed_domain(url):
            logging.warning(f"Blocked out-of-scope URL: {url}")
            return False

        # ToS compliance check
        if domain not in self.tos_allow:
            self.load_terms_of_service(domain)
        if not self.tos_allow.get(domain, True):
            logging.warning(f"Blocked by ToS: {url}")
            return False

        # robots.txt check
        rp = self._get_robots_parser(domain)
        if not rp.can_fetch(self.session.headers['User-Agent'], url):
            return False

        # Deduplication checks
        try:
            result = self.visited_db.execute("SELECT 1 FROM urls WHERE url=?", (url,)).fetchone()
            if result:
                return False
        except sqlite3.Error as e:
            logging.error(f"URL check failed: {e}")

        try:
            with self.db_cursor() as cursor:
                cursor.execute("SELECT 1 FROM targets WHERE url = %s", (url,))
                return not cursor.fetchone()
        except Exception as e:
            logging.error(f"DB check failed: {e}")
            return False

    def load_terms_of_service(self, domain: str):
        """Fetch and analyze domain's terms of service"""
        try:
            tos_urls = [
                f"https://{domain}/terms",
                f"https://{domain}/terms-of-service",
                f"https://{domain}/tos",
                f"https://{domain}/legal"
            ]
            
            for url in tos_urls:
                try:
                    response = self.session.get(url, timeout=10, verify=True, stream=True)
                    if response.status_code == 200:
                        content = ""
                        for chunk in response.iter_content(2048, decode_unicode=True):
                            content += chunk
                            if len(content) > 50000:  # Limit to 50KB
                                break
                        
                        if any(pattern.search(content) for pattern in self.tos_patterns):
                            self.tos_allow[domain] = False
                            return
                        self.tos_allow[domain] = True
                        return
                except requests.RequestException:
                    continue

            # Default to blocked if no ToS found
            logging.warning(f"No ToS found for {domain}. Defaulting to block.")
            self.tos_allow[domain] = False
        except Exception as e:
            logging.error(f"Error loading ToS for {domain}: {e}")
            self.tos_allow[domain] = False

    def run(self, target_url=None, bugcrowd_pages=3):
        self._save_scan_start()
        
        if target_url:
            self.domain_restriction = urlparse(target_url).netloc
            self.queue_url(target_url)
        else:
            self.scrape_bugcrowd_programs(pages=bugcrowd_pages)

        while self.stats['pages_scraped'] < self.stats['max_pages']:
            batch = self._get_next_batch()
            if not batch:
                break
            
            for url in tqdm(batch, desc="Scanning URLs"):
                if response := self.make_request(url):
                    self.process_response(response, url)
            
            self._mark_batch_processed(batch)
            self._flush_buffers()

        self._shutdown_cleanup()

    def make_request(self, url: str) -> Optional[requests.Response]:
        parsed = urlparse(url)
        cb = self._get_circuit_breaker(parsed.netloc)
        
        if cb.should_block():
            CIRCUIT_STATE.labels(domain=parsed.netloc).set(1)
            return None

        try:
            start_time = time.time()
            response = self.session.get(
                url,
                timeout=15,
                allow_redirects=True,
                verify=True,
                proxies=self._get_proxies(),
                stream=True
            )
            duration = time.time() - start_time
            
            REQUEST_DURATION.labels(domain=parsed.netloc).observe(duration)
            REQUESTS_TOTAL.labels(domain=parsed.netloc, status=response.status_code).inc()
            
            response.raise_for_status()
            return response
            
        except requests.RequestException as e:
            cb.record_failure()
            CIRCUIT_STATE.labels(domain=parsed.netloc).set(0)
            REQUESTS_TOTAL.labels(domain=parsed.netloc, status='error').inc()
            logging.error(f"Request failed: {e}")
            return None

    def process_response(self, response: requests.Response, url: str):
        if not self._validate_content_type(response):
            return
        
        content_type = response.headers.get('Content-Type', '')
        content = response.text[:5_000_000]  # Limit to 5MB
        
        if 'html' in content_type:
            self.process_html(content, url)
        elif 'json' in content_type:
            self.process_json(content, url)
        
        self._save_page_scan(url)
        self.stats['pages_scraped'] += 1
        QUEUE_SIZE.set(len(self.url_queue))

    def _validate_content_type(self, response) -> bool:
        content_type = response.headers.get('Content-Type', '')
        extension = urlparse(response.url).path.split('.')[-1].lower()
        
        forbidden_types = {'octet-stream', 'image/', 'video/'}
        if any(ft in content_type for ft in forbidden_types):
            return False
            
        if 'javascript' in content_type and extension not in ('js', 'json'):
            return False
            
        return True

    def _shutdown_cleanup(self):
        self._flush_buffers()
        self._save_scan_end()
        self.pool.closeall()
        self.visited_db.close()

if __name__ == "__main__":
    DB_CONFIG = {
        'dbname': 'bugbounty',
        'user': 'scraper',
        'password': 'securepassword',
        'host': 'localhost',
        'port': '5432'
    }

    scraper = BugBountyScraper(
        db_config=DB_CONFIG,
        allowed_domains={'bugcrowd.com', 'api.bugcrowd.com'},
        use_proxies=False
    )
    
    scraper.run(target_url="https://example.com")
